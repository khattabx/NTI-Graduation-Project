##! Define names for each Flume component
agent.sources = log-source           # Naming the source as "log-source"
agent.sinks = hdfs-sink              # Naming the sink as "hdfs-sink"
agent.channels = memory-channel      # Naming the channel (buffer between source and sink) as "memory-channel"
##! Source Configuration: Read from local log file using tail -F
agent.sources.log-source.type = exec
agent.sources.log-source.command = tail -F /home/bigdata/Desktop/logGenerator/logs/sample.log
# Flume will continuously monitor the local sample.log file using tail -F
# Every new line appended to sample.log will be captured by Flume in real-time
# Channel Configuration: Use in-memory buffer
agent.channels.memory-channel.type = memory
agent.channels.memory-channel.capacity = 1000                # Maximum number of events the channel can store in memory
agent.channels.memory-channel.transactionCapacity = 100      # Maximum number of events to process in a single transaction
##! Sink Configuration: Write events to HDFS
agent.sinks.hdfs-sink.type = hdfs
agent.sinks.hdfs-sink.hdfs.path = hdfs://localhost:9000/user/bigdata/graduation/
##! Destination path in HDFS where log data will be saved
agent.sinks.hdfs-sink.hdfs.fileType = DataStream
##! DataStream means events are written to HDFS directly without large file buffering
agent.sinks.hdfs-sink.hdfs.writeFormat = Text
##! Events will be written as plain text (one log line per line in HDFS)
agent.sinks.hdfs-sink.hdfs.rollInterval = 10
# A new HDFS file is created every 10 seconds (even if the file is small)
##! Connect Source to Channel
agent.sources.log-source.channels = memory-channel
##! Connect Sink to Channel
agent.sinks.hdfs-sink.channel = memory-channel